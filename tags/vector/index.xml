<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>vector on GuangzhengLi</title><link>https://guangzhengli.com/tags/vector/</link><description>Recent content in vector on GuangzhengLi</description><generator>Hugo -- gohugo.io</generator><language>zh</language><managingEditor>iguangzhengli@gmail.com (Guangzheng Li)</managingEditor><webMaster>iguangzhengli@gmail.com (Guangzheng Li)</webMaster><copyright>2023 GuangzhengLi All rights reserved</copyright><lastBuildDate>Mon, 31 Jul 2023 20:10:00 +0800</lastBuildDate><atom:link href="https://guangzhengli.com/tags/vector/index.xml" rel="self" type="application/rss+xml"/><item><title>GPT 应用开发和思考</title><link>https://guangzhengli.com/blog/zh/gpt-embeddings/</link><pubDate>Mon, 31 Jul 2023 20:10:00 +0800</pubDate><author>iguangzhengli@gmail.com (Guangzheng Li)</author><guid>https://guangzhengli.com/blog/zh/gpt-embeddings/</guid><description>&lt;p>在过去几个月的时间中，我们似乎正处于人工智能的革命中。除了大多数人了解的 OpenAI ChatGPT 之外，许多非常新颖、有趣、实用的 AI 应用也是层出不穷，并且在使用这些应用时时，笔者也确确实实的感受到了生产力的提高。&lt;/p>
&lt;p>但是关于 GPT 应用的开发知识和路线，目前似乎还没有太多的资料，所以笔者决定将自己的一些经验和思考整理成一个系列，希望能够帮助到大家。&lt;/p>
&lt;p>本篇文章主要介绍的是 GPT 相关应用的开发思考，在今年 4 月份的时候，笔者因为开发 &lt;a href="https://github.com/guangzhengli/ChatFiles">ChatFiles&lt;/a> 这个开源项目，从而学习了 GPT 相关的技术知识，但是由于笔者的时间精力有限，所以一直没有机会将这些知识整理成一篇文章，直到最近笔者又因为有了新的想法，开源了 &lt;a href="https://github.com/guangzhengli/vectorhub">VectorHub&lt;/a> 这个同样基于 GPT Prompt 和 Embeddings 技术的项目，进而对 GPT 和 Embeddings 等技术知识有了更深入的了解，所以就有了这一篇分享。&lt;/p>
&lt;h2 id="从-prompt-开始">从 Prompt 开始&lt;/h2>
&lt;p>AI 应用开发在过去一段时间内吸引了众多开发者入场，除了大家所熟知的 ChatGPT 之外，还涌现了大量有实际价值的 AI 应用，例如基于 AI 的翻译类的应用如 &lt;a href="https://github.com/openai-translator/openai-translator">openai-translator&lt;/a>、&lt;a href="https://immersivetranslate.com/">immersivetranslate&lt;/a>，写作类的应用如 &lt;a href="https://www.notion.so/product/ai">Notion AI&lt;/a>，编程辅助类的应用如 &lt;a href="https://github.com/features/copilot">GitHub Copilot&lt;/a> 和 &lt;a href="https://docs.github.com/en/copilot/github-copilot-chat/using-github-copilot-chat?tool=vscode">GitHub Copilot Chat&lt;/a>。&lt;/p>
&lt;p>这些应用有些是优化了原有的体验，如基于 GPT 的翻译的 &lt;a href="https://github.com/openai-translator/openai-translator">openai-translator&lt;/a>，翻译质量和阅读体验远胜于之前的机器翻译，还有些则是提供了之前无法实现的功能，如 &lt;a href="https://github.com/features/copilot">GitHub Copilot&lt;/a> 的代码补全和生成，还有像 &lt;a href="https://docs.github.com/en/copilot/github-copilot-chat/using-github-copilot-chat?tool=vscode">GitHub Copilot Chat&lt;/a> 提供回答编码相关问题、解释代码、生成单元测试、给错误代码提出修复意见等等功能，这些功能的实现难度在以前是完全无法想象的。&lt;/p>
&lt;p>这些应用在功能上虽然没有相似之处，但是在实现原理中，它们都是主要基于 GPT 的 Prompt(提示)实现。Prompt 指的是提供给模型的文本或指令，可以用来引导模型生成自然语言输出(Completion)。它可以给模型提供上下文信息，对模型的输出结果至关重要。&lt;/p>
&lt;p>我们知道 GPT（Generative Pre-trained Transformer）是一个推理模型，它主要基于预训练和微调两个阶段。&lt;/p>
&lt;p>在预训练阶段会使用一个大规模的语料库进行基础训练，例如使用维基百科、新闻文章、小说等来进行训练。当训练完成后，输入一句话给它，它会基于这句话给出一个概率上的预测，预测后续应该拼接上什么单词，这个拼接的单词是基于它在预训练阶段学习到的知识来进行概率选定的，通过一次次循环的单词预测，最终可以拼接出一段话来。这也是它被称为生成式 AI 的原因。&lt;/p>
&lt;p>这一句话就是我们所说的 Prompt，也是生成式 AI 概率生成的基础。这能解释为什么我们每次输入相同的 Prompt，但是每次得到的结果都会有所不同，因为每次的结果都是基于概率生成的。&lt;/p>
&lt;p>所以我们也就能理解为什么 Prompt 对于 GPT 应用开发的重要性，因为它是除了微调以外，我们能与 GPT 模型唯一的交互方式（当然除此之外还可以通过调整模型的 temperature 和 top_p 两个配置来控制 GPT 更多样化或更具创造性的输出，不过对于输出质量和对下游任务的处理能力并无明显影响）。所以 Prompt 是 GPT 应用开发最核心的部分，也是最需要开发者去思考和优化的部分。&lt;/p>
&lt;p>在预训练完成后，在微调阶段会将 GPT 模型加载到特定的任务上，并使用该任务的数据集对模型进行训练。这样，模型就可以根据任务的要求进行微调，以便更好地理解 Prompt 并生成与任务相关的文本。通过微调，GPT 可以适应不同的任务，如文本分类、情感分析、问答系统等。不过目前微调因为成本昂贵和最终效果并不稳定，对于广大 GPT 开发者来讲，并不是非常好的选择，所以目前大部分的 GPT 应用开发者都是基于 Prompt 来开发应用。&lt;/p>
&lt;h2 id="prompt-学习路线">Prompt 学习路线&lt;/h2>
&lt;p>关于 Prompt 的基础知识，可以先去看看吴恩达老师的 &lt;a href="https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/">ChatGPT Prompt 工程&lt;/a>。可以通过两个小时不到的视频可以快速的了解 Prompt 的使用方式和它的魅力所在。&lt;/p>
&lt;p>在有了一个初步的了解之后，笔者推荐 &lt;a href="https://www.promptingguide.ai/">Prompt Engineering Guide&lt;/a> 这份文档，该文档包含了大量的 Prompt 基础知识和未来的发展方向，对于 GPT 应用开发者来说，除了学习 Prompt 的基础知识之外，还可以从中获取到一些工程界和学术界对于 Prompt 的发展方向的思考，对于开发 AI 应用来说，这些思考弥足珍贵。&lt;/p>
&lt;p>最后，非常推荐大家去看看 OpenAI 官方的 &lt;a href="https://platform.openai.com/docs/guides/gpt-best-practices">GPT 最佳实践&lt;/a> 这份文档，它是由 OpenAI 官方提供的 GPT 最佳实践指南，里面包含了大量的 Prompt 示例和使用技巧，对于 GPT 应用开发者来说，是一份非常有价值的文档。因为它是 OpenAI 官方通过合作伙伴或者 Hackathon 等不同的方式，在不同的业务领域 GPT 应用开发中总结出来的最佳实践，对于开发者来讲非常有启发价值！下面是对这一份文档的摘录：&lt;/p>
&lt;blockquote class="twitter-tweet">&lt;p lang="zh" dir="ltr">撰写清晰的 Prompt&lt;br>&lt;br>1⃣️：在问题中包含详细的信息，例如以下例子：&lt;br>&lt;br>不好的提问：总结会议记录。&lt;br>优秀的提问：将会议记录总结成一个段落。然后用 Markdown 列表的格式列举一下参与者们各自的要点。最后如果有的话，列出演讲者建议的下一步行动或行动项目。&lt;br>&lt;br>不好的提问：编写代码来计算斐波那契数列。…&lt;/p>&amp;mdash; Guangzheng Li (@iguangzhengli) &lt;a href="https://twitter.com/iguangzhengli/status/1668059021390266369?ref_src=twsrc%5Etfw">June 12, 2023&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;h2 id="prompt-最佳实践">Prompt 最佳实践&lt;/h2>
&lt;p>关于 Prompt 编写的最佳实践，最为推荐的当然是 OpenAI 官方出品的 &lt;a href="https://platform.openai.com/docs/guides/gpt-best-practices">GPT 最佳实践&lt;/a> 这份文档，但对于开发 GPT 应用来讲，笔者还是想结合这份最佳实践和一些自己的经验，给大家分享一些 GPT 开发的实践。&lt;/p>
&lt;h3 id="清晰和详细">清晰和详细&lt;/h3>
&lt;p>现实中大部分开发者在平常使用 GPT 的时候，都是以解决编程问题或者询问问题为主，所以容易带入以往使用 Google 等搜索引擎的经验来使用和开发 GPT。&lt;/p>
&lt;p>例如当你想知道如何用 Python 编写斐波那契数列的时候，如果在以前使用 Google 搜索引擎，你可能会输入 &lt;code>python fibonacci&lt;/code>。因为这样就足够了，Google 是基于倒排索引和 PageRank 算法的，只需要输入关键字，就能得到高质量的网页答案。&lt;/p>
&lt;p>所以只需要这种输入两个字的输入方式是最简单和最高效的，毕竟就算多输入几个字 &lt;code>how to write python fibonacci&lt;/code> ，对于 Google 搜索引擎来讲，输出质量是相差不大的。&lt;/p>
&lt;p>而如果你使用的是 GPT，像 &lt;code>python fibonacci&lt;/code> 这样的输入对于 GPT 来说是非常不友好的，因为它无法清晰的理解你的意图，所以它可能会给出一些不相关的结果（根据不同模型的质量会略有差异）。&lt;/p>
&lt;p>但如果你输入的是 &lt;code>编写一个用 python 函数来高效地计算 fibonacci 数列。评论每一行代码以解释每一部分的作用以及为什么这样写&lt;/code>。这样的输入对于 GPT 来说就非常清晰和详细，它能够清晰的理解你的意图，所以它给出的结果也会更加准确，&lt;strong>在保证输出质量下限的同时，还可以提高输出质量的上限！&lt;/strong>&lt;/p>
&lt;p>这和开发者以往使用 Google 等搜索引擎的经验是完全不同的，也是 GPT 开发者和使用者最容易忽视的地方。笔者在今年很早的时候开发 &lt;a href="https://github.com/guangzhengli/ChatFiles">ChatFiles&lt;/a> 项目时，曾匿名收集过使用者的 Prompt，发现 95% 以上的使用者使用的都是非常简单的 Prompt，简单的甚至看起来有一种一字千金的感觉。&lt;/p>
&lt;p>所以当开发者开发 GPT 应用时，一定要注意 Prompt 的清晰和详细，多试几次，选择一个输出质量稳定和格式相同的 Prompt，这是保证 GPT 应用质量的关键。&lt;/p>
&lt;h3 id="如何处理更加复杂的任务">如何处理更加复杂的任务&lt;/h3>
&lt;p>相信所有开发者面对简单的场景任务，多花费一些时间来调整 Prompt，都能设计好程序的 Prompt 并得到不错的输出质量。但是面对复杂任务时，想要提高 GPT 的输出质量还需要两个非常重要的技巧：&lt;strong>让 GPT 推理而不是回答，和拆分任务进行引导&lt;/strong>。&lt;/p>
&lt;h4 id="推理而不是回答">推理而不是回答&lt;/h4>
&lt;p>推理而不是回答是指在 Prompt 中，要求 GPT 模型不要立即判断正确与否或者立刻给出答案，而是引导模型进行深入思考。可以要求其先列出对问题的各种看法，对任务进行拆分，说明每一步的推理依据，然后再得出最终结论。在 Prompt 中添加逐步推理的要求，能让语言模型投入更多时间逻辑思维，输出结果也将更可靠准确。&lt;/p>
&lt;p>举一个&lt;a href="https://platform.openai.com/docs/guides/gpt-best-practices/tactic-instruct-the-model-to-work-out-its-own-solution-before-rushing-to-a-conclusion">OpenAI 官方的例子&lt;/a>，如果你需要 GPT 回答某一个学生的答案是否正确，Prompt 是 &lt;code>判断学生的解决方案是否正确&lt;/code>的话，面对复杂的计算问题和答案，GPT 有很大的概率会给出错误的答案，因为 GPT 并不会像人一样先进行推理答案再进行回答，而是会立即给出判断。在短暂的判断中，就无法给出正确的答案（就好比人类无法在短时间计算复杂数学一样）。&lt;/p>
&lt;p>所以如果我们的 Prompt 是 &lt;code>首先自己解决问题，然后再将自己的解决方案与学生的解决方案进行比较，并评估学生的解决方案是否正确。在自己完成问题之前，不要确定学生的解决方案是否正确&lt;/code>。在 Prompt 中给出明确的引导和条件，就能够让 GPT 模型花费更多的时间推导答案，从而得到更加准确的结果。&lt;/p>
&lt;h4 id="拆分任务">拆分任务&lt;/h4>
&lt;p>拆分任务进行引导是指将一个复杂的任务拆分成多个子任务，然后分别引导 GPT 模型进行推理，最后将多个子任务的结果进行整合，得到最终的结果。这样做的好处是可以让 GPT 模型更加专注于一个子任务，从而提高输出质量。&lt;/p>
&lt;p>举一个不太恰当的例子，当你需要对一本书籍进行摘要的时候，GPT 直接进行总体的摘要的效果并不好，我们可以使用一系列子任务来摘要每个部分。最后再汇总产生的摘要。&lt;/p>
&lt;p>当然拆分任务也会带来一些新的问题，即当单个任务输出质量有问题时，整体的输出质量也会受其影响。加上目前的 token 费用不菲，拆分任务进行引导也会带来额外的成本。但是无论如何，目前关于如何设计和拆分复杂任务是所有 GPT 应用最需要思考，和维护自身 AI 应用护城河的核心问题，也是目前大模型 AI 框架像 LangChain 等项目的核心设计点，有空可以单独写一篇文章来讨论。&lt;/p>
&lt;h3 id="使用技巧">使用技巧&lt;/h3>
&lt;p>除了上述的一些比较重要的实践之外，还有一些小技巧对于开发应用也是非常有帮助，下面是笔者总结的一些使用技巧：&lt;/p>
&lt;ul>
&lt;li>提供少量示例(Few-Shot Prompting): 给模型一两个期望的输入输出样例，让模型了解我们的要求和期望的输出样式。&lt;/li>
&lt;li>Prompt 中要求结构化输出：以 Json 的方式输出，这样可以方便后续代码程序处理。&lt;/li>
&lt;li>分隔符：使用分隔符像 &lt;code>&amp;quot;&amp;quot;&amp;quot;&lt;/code> 将不同的指令、上下文之间进行隔离，防止系统的 Prompt 和用户输入的 Prompt 混淆冲突。&lt;/li>
&lt;/ul>
&lt;h2 id="gpt-embedding-应用开发">GPT Embedding 应用开发&lt;/h2>
&lt;p>在上面我们主要介绍了基于 Prompt 如何开发 AI 应用，但是有时候我们还会遇到新的问题。例如大模型的训练数据往往是基于几个月前、甚至是几年前的，当我们有一些需求场景是需要 GPT 应用提供最新的数据，例如基于最近的新闻来回答问题，或者根据私有文档来回答问题，这时候大模型本身就没有基于这些材料进行过训练，也就无法解决该类问题。&lt;/p>
&lt;p>这个时候我们可以通过让模型使用参考文本来回答问题，例如我们的 prompt 可以写成:&lt;/p>
&lt;pre tabindex="0">&lt;code>你将会得到一个由三个引号分隔的文档和一个问题。
你的任务是使用提供的文档回答问题，并引用用于回答问题的文本段落。
如果文档中没有包含回答该问题所需的信息，则简单地写下：“信息不足”。
如果提供问题的答案，必须附带引用注释。
使用以下格式引用相关段落（{&amp;#34;citation&amp;#34;: …}）。
&lt;/code>&lt;/pre>&lt;p>这种使用方式可以让 GPT 针对于我们给予它的参考文本进行回答，例如我们想问最新的世界杯冠军是谁，就可以附上最新的世界杯新闻的参考文本，这样 GPT 会先理解整篇新闻，再来问答问题。通过这种方式，就可以解决大模型针对于时效性和特定的下游任务的问题。&lt;/p>
&lt;p>但是这种解决方案会带来另外一个问题，即参考文本长度限制的问题。GPT Prompt 是有大小限制的，像 gpt-3.5-turbo 模型它的限制是 4K tokens(～3000字)，这意味着使用者最多只能输入 3000 字给 GPT 来理解和推理答案。&lt;/p>
&lt;p>那么一旦需要的参考文本大于 3000 字，就无法一次性通过 GPT 得到答案，需要开发者想办法将参考文本拆分成多个部分，然后分别进行 GPT Embeddings 转化为向量存储到向量数据库中。关于向量数据库的更多信息，可以参考我另外一篇博客 &lt;a href="https://guangzhengli.com/blog/zh/vector-database/">向量数据库&lt;/a>，当需要针对于参考文本进行提问时，需要先将问题转化为向量，然后通过向量数据库进行检索，最后再将检索到的向量转化为文本输出。这样就能得到符合 GPT tokens 限制的参考文本，并且这段参考文本是和问题具有关联的。&lt;/p>
&lt;p>我们拿到问题本身和这段具有关联的参考文本，同时提交给 GPT，就能得到我们想要的答案。这个过程就是 GPT Embeddings 应用开发核心，它的核心思想是通过向量检索的方式检索与问题最相关的文本段，从而绕过 GPT tokens 的限制。&lt;/p>
&lt;p>&lt;img src="https://storage.guangzhengli.com/images/Embedding.png" alt="Embedding">&lt;/p>
&lt;p>整体的开发流程如上图所示：&lt;/p>
&lt;ol>
&lt;li>加载文档，获取目标文本信息。例如 LLM 主流框架 LangChain 中 File Loader 和 Web Loader 的两种文本获取方式。
&lt;ol>
&lt;li>基于文件系统的 File Loader，如加载 PDF 文件、Word 文件等。&lt;/li>
&lt;li>基于网络的 Web Loader，如某个网页，AWS S3 等。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>将目标文本拆分为多个段落，拆分方式主要基于两种。
&lt;ol>
&lt;li>一种是基于文字数量的拆分法，例如 1000 字为一段，这种方式的优点是简单，缺点是可能会将一个段落拆分成多个段落，导致段落的连贯性变差，从而导致最终回答结果可能缺少上下文而降低回答质量。&lt;/li>
&lt;li>另一种是基于标点符号的拆分法，例如以换行为分隔符，这种方式的优点是段落的连贯性好，缺点是每个段落大小不一，可能会导致触发 GPT tokens 的限制。&lt;/li>
&lt;li>最后一种是基于 GPT tokens 限制的拆分法，例如以 2000 tokens 为一组，最终查询时搜索出最相关的两个段落，这样加到一起也才 4000 tokens,就不会触发到 4096 tokens 的限制。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>将拆分的文本块统一存入 &lt;a href="https://guangzhengli.com/blog/zh/vector-database/">向量数据库&lt;/a> 中。&lt;/li>
&lt;li>将用户问题转化为向量，然后通过向量数据库进行检索，得到最相关的文本段落。（需要注意的是，这里的检索并不是传统数据库的模糊匹配或者倒排索引，而是基于语义化的搜索能力，所以检索出的文本才能回答用户问题，详细请看另外一篇博客 [向量数据库](https://guangzhengli.com/blog/zh/vector-database/）&lt;/li>
&lt;li>将检索出来的相关文本信息，用户问题和系统的 prompt 三个组合成一个针对于 Embeddings 场景的 Prompt，例如 Prompt 中明确写到使用参考文本回答问题，而不是 GPT 自己回答。&lt;/li>
&lt;li>GPT 回答最终得到的 Prompt，得到最终的答案。&lt;/li>
&lt;/ol>
&lt;p>如果你对具体的实现代码感兴趣，可以去看看 LangChain 中关于 &lt;a href="https://js.langchain.com/docs/modules/data_connection/">Retrieval 的章节&lt;/a>。&lt;/p>
&lt;p>GPT embeddings 应用能解决某些场景下 GPT 无法回答的问题，这是他最大的优点。无需训练，无需微调，只需要将文本转化为向量再通过检索就能实现，以低成本的方式就能让某些业务场景变成可能。&lt;/p>
&lt;p>但这个方案同样会带来一些新的问题，例如 embeddings 文本拆分和检索的质量将在很大的程度上影响最终的结果，还有查询范围、质量和查询时间该如何均衡，检索出的参考文本无法回答用户问题时该如何处理？这都是开发者在面对业务需求和场景时需要仔细思考的问题。&lt;/p>
&lt;p>如果思考更远一点，在人类过去的时间中，所有的文档都是面向人来编写，对于向量检索来说并不是最佳组织模式，以后是否会存在专门面向 AI 和检索编写的文本？让 AI 更好理解和更好符合数据库检索？不过这些问题需要长期的思考和论证，这里不再扩展。&lt;/p>
&lt;h2 id="gpt-agents-应用开发">GPT Agents 应用开发&lt;/h2>
&lt;p>除了上述的 prompt 和 embeddings 两种方案能解决的业务需求外，GPT 应用开发还有一个非常常见的需求，即如何集成现有系统，或者说是如何集成现有 API。&lt;/p>
&lt;p>因为软件行业已经发展了很多年，很多公司都有自己的系统和 API，这些 API 能够极大的提高 GPT 应用的能力边界。如果 GPT 应用想要在实际生活场景中落地，是不可避免的需要和现有系统进行集成。&lt;/p>
&lt;p>就像如果你想要询问 GPT 一个非常简单的问题，今天北京的天气是什么？看完上面的章节，我们知道 GPT 自己是无法回答这类问题的，如果 GPT 能够自行调用某个天气 API，那么开发起来将非常的方便。&lt;/p>
&lt;p>想要实现 GPT 调用查询天气这个 API 的需求，我们会面临两个问题，一个是让 GPT 应用理解某个 API 的功能，并在它觉得合适的时候调用；另一个是输入输出必须结构化，从而保证系统的稳定性。&lt;/p>
&lt;h3 id="理解和调用现有的-api">理解和调用现有的 API&lt;/h3>
&lt;p>想让 GPT 理解某个 API 的功能，最佳的方式当然是开发者手动给 API 添加名字和具体的描述信息，包括输入和输出值的结构，和每个字段代表着什么意思，这能很大程度上影响 GPT 的判断，最后决定是否调用该 API。&lt;/p>
&lt;p>如 OpenAI 官网中 &lt;a href="https://openai.com/blog/function-calling-and-other-api-updates">function calling 例子&lt;/a>中，关于调用天气 API 的描述如下所示。&lt;/p>
&lt;pre tabindex="0">&lt;code>function_descriptions = [
{
&amp;#34;name&amp;#34;: &amp;#34;get_current_weather&amp;#34;,
&amp;#34;description&amp;#34;: &amp;#34;Get the current weather in a given location&amp;#34;,
&amp;#34;parameters&amp;#34;: {
&amp;#34;type&amp;#34;: &amp;#34;object&amp;#34;,
&amp;#34;properties&amp;#34;: {
&amp;#34;location&amp;#34;: {
&amp;#34;type&amp;#34;: &amp;#34;string&amp;#34;,
&amp;#34;description&amp;#34;: &amp;#34;The city and state, e.g. San Francisco, CA&amp;#34;,
},
&amp;#34;unit&amp;#34;: {
&amp;#34;type&amp;#34;: &amp;#34;string&amp;#34;,
&amp;#34;description&amp;#34;: &amp;#34;The temperature unit to use. Infer this from the users location.&amp;#34;,
&amp;#34;enum&amp;#34;: [&amp;#34;celsius&amp;#34;, &amp;#34;fahrenheit&amp;#34;]
},
},
&amp;#34;required&amp;#34;: [&amp;#34;location&amp;#34;],
},
}
]
&lt;/code>&lt;/pre>&lt;p>上面的方法详细描述了如果需要调用该函数，至少需要提供当前所在的位置信息，并且可以选择性的提供温度单位。GPT 会根据问题是否和该函数的描述相关联，来决定是否调用该函数。具体的代码细节以后有几乎再写一篇，暂时不展开。&lt;/p>
&lt;p>我们可以通过提供大量的 APIs 的说明文档，让 LLMs 理解并学会这些 APIs 的功能、使用方式、组合方式。最终达到根据提出的总体要求，将总体要求拆分为多个子任务，具体子任务再与某个 API 进行交互，最终达到自动化以及 AI 的目标。&lt;/p>
&lt;p>我们这里描述的现有 APIs 并不仅仅是基于 HTTP 的请求，可以包括转化为 SQL 查询数据库，调用 SDK 实现复杂功能，甚至在未来，可以扩展为调用物理世界里面的开关、机械臂等。个人认为，基于目前已有的 GPT 能力和发展趋势，人机交互的方式将产生巨大的变化。&lt;/p>
&lt;h3 id="结构化输出">结构化输出&lt;/h3>
&lt;p>除了让 GPT 理解和调用现有的 API 之外，还需要让 GPT 输出的结果能够被现有系统所理解，这就需要 GPT 输出的结果是结构化的，例如 JSON 格式的数据。&lt;/p>
&lt;p>那么你可能马上想到，我们通过 prompt 不就可以实现这种功能吗？在大部分情况下确实可以，但是有些少量的情况，prompt 并不如 function calling 的方式稳定，而我们传统的系统，对于稳定性的要求是非常高的，举个例子。&lt;/p>
&lt;pre tabindex="0">&lt;code>student_description=小王是北京大学计算机科学专业的二年级学生，绩点为 3.8 GPA。他的编程很厉害，是该大学机器人俱乐部的活跃成员。他希望在毕业后追求人工智能方面的工作。
&lt;/code>&lt;/pre>&lt;p>这段话中，我们可以通过 prompt 来要求输出的结构为 json 格式，例如 prompt 为&lt;/p>
&lt;pre tabindex="0">&lt;code>Please extract the following information from the given text and return it as a JSON object:
name
major
school
grades
club
This is the body of text to extract the information from:
{student_1_description}
&lt;/code>&lt;/pre>&lt;p>但是比较难处理的是，我们没有办法能够确定 GPT 最终会输出的 grades 是 &lt;code>3.8&lt;/code> 还是 &lt;code>3.8 GPA&lt;/code>，这两种输出结果对于人类来讲没有如何区别，但是对于计算机来讲，这两种输出结果是完全不同的，前者是一个浮点数，后者是一个字符串。对于某些语言来讲转换就会直接出错。&lt;/p>
&lt;p>我们当然可以通过增加 prompt 的描述来减少这类问题，但是现实情况是比较复杂的，我们很难通过自然语言完全描述清楚需求，很难保证 GPT 的回答会每次都保持相同的输出。所以针对于这类问题，通过 OpenAI 提供的 function calling 的方式，能够在一定程度上解决自然语言与机器语言交互的问题。&lt;/p>
&lt;p>就像上述的问题可以描述为一个函数，我们将 grades 这个字段描述为 integer 来避免这类问题。这种结构化的能力对于开发一个稳定的系统是至关重要的。&lt;/p>
&lt;pre tabindex="0">&lt;code>student_custom_functions = [
{
&amp;#39;name&amp;#39;: &amp;#39;extract_student_info&amp;#39;,
&amp;#39;description&amp;#39;: &amp;#39;Get the student information from the body of the input text&amp;#39;,
&amp;#39;parameters&amp;#39;: {
&amp;#39;type&amp;#39;: &amp;#39;object&amp;#39;,
&amp;#39;properties&amp;#39;: {
&amp;#39;name&amp;#39;: {
&amp;#39;type&amp;#39;: &amp;#39;string&amp;#39;,
&amp;#39;description&amp;#39;: &amp;#39;Name of the person&amp;#39;
},
&amp;#39;major&amp;#39;: {
&amp;#39;type&amp;#39;: &amp;#39;string&amp;#39;,
&amp;#39;description&amp;#39;: &amp;#39;Major subject.&amp;#39;
},
&amp;#39;school&amp;#39;: {
&amp;#39;type&amp;#39;: &amp;#39;string&amp;#39;,
&amp;#39;description&amp;#39;: &amp;#39;The university name.&amp;#39;
},
&amp;#39;grades&amp;#39;: {
&amp;#39;type&amp;#39;: &amp;#39;integer&amp;#39;,
&amp;#39;description&amp;#39;: &amp;#39;GPA of the student.&amp;#39;
},
&amp;#39;club&amp;#39;: {
&amp;#39;type&amp;#39;: &amp;#39;string&amp;#39;,
&amp;#39;description&amp;#39;: &amp;#39;School club for extracurricular activities. &amp;#39;
}
}
}
}
]
&lt;/code>&lt;/pre>&lt;p>完整的调试过程可以去看看这个&lt;a href="https://www.datacamp.com/tutorial/open-ai-function-calling-tutorial">OpenAI function calling 的例子&lt;/a>。&lt;/p>
&lt;h2 id="gpt-应用需求分析">GPT 应用需求分析&lt;/h2>
&lt;p>上面主要是关于如何开发 GPT 应用的一些技巧，但是如果想要创造产品，我们还是需要从业务需求出发，去思考我们能够创造出什么样的业务价值，从而满足当前潜在用户的需求。&lt;/p>
&lt;blockquote>
&lt;p>2023/09/06 更新：LangChain 官方文档更新，将文档分为 RAG(RetrievalAugmentedGeneration) 和 Agents 两部分。这说明从 GPT 爆发几个月以来，工业界目前也觉得 RAG 和 Agents 是业务需求落地的两个方向。其中 RAG 就是我们上面主要讲的 GPT Embeddings 部分，后续我们会也从这两个方向举例说明。我觉得锚定像 langchain 这样的头部框架对于开发者来讲是非常有价值的，因为它们会在业务落地的过程中，总结出一些最佳实践和经验，这些经验对于开发者理解业务需求有很帮助。&lt;/p>
&lt;/blockquote>
&lt;h3 id="内容生成">内容生成&lt;/h3>
&lt;p>内容生成是目前最主流的需求，也是当前 AI 应用中流量统计最高的一个方向。除了 ChatGPT 这样大家所知的产品外，还有像 &lt;a href="https://beta.character.ai/">Character AI&lt;/a> 这样主要基于 Prompt 开发的人工智能伴侣（角色扮演类）应用流量也是非常的高。&lt;/p>
&lt;p>在更广泛的内容生成类中，图像生成领域如 &lt;a href="https://www.midjourney.com/">Midjourney&lt;/a>，语音生成领域如 &lt;a href="https://elevenlabs.io">ElevenLabs&lt;/a>。文字创意类领域如 &lt;a href="https://www.copy.ai/">copy ai&lt;/a>，还包括一些细分领域的文字内容生成，如小说生成辅助 &lt;a href="https://ai-novel.com/">AI-Novel&lt;/a> 等等。&lt;/p>
&lt;p>内容生成是当前互联网流量最高的 AI 需求领域，也是最容易落地的领域，因为它的应用场景非常广泛，内容生成辅助往往能够直接提高生产力，所以付费意愿往往不低，对于这个领域的争夺，往往也是最激烈的。&lt;/p>
&lt;h3 id="gpt-embeddings-需求">GPT Embeddings 需求&lt;/h3>
&lt;p>GPT Embeddings 是我觉得是非常有潜在价值的方向，其中 &lt;a href="https://github.com/guangzhengli/ChatFiles">ChatFiles&lt;/a> 刚开源不久的时候，我就收到了一些的咨询，问能否在客服、销售、操作手册、知识库等场景下优化现有场景和业务，我个人的回答是 GPT Embeddings 在这些场景下是非常有前景的。&lt;/p>
&lt;p>这个领域目前能看到的创业企业像 &lt;a href="https://www.mendable.ai">mendable ai&lt;/a> 就已经占据了一定的市场份额，支持了像 &lt;a href="https://js.langchain.com/">LangChain&lt;/a> 等头部 GPT 框架的文档问答功能。并且还在积极扩展销售、客户等业务场景。&lt;/p>
&lt;p>除此之外，这个领域目前流量最高的当属于 &lt;a href="https://www.chatpdf.com">ChatPDF&lt;/a> 这个网站，可以上传 PDF 文件，然后基于 PDF 提出问题和需求如总结等。这个方向也衍生出各个细分领域的需求，如辅助论文阅读和协作的像 &lt;a href="https://jenni.ai">Jenni AI&lt;/a> 等。&lt;/p>
&lt;h3 id="gpt-agents-需求">GPT Agents 需求&lt;/h3>
&lt;p>GPT Agents 的需求五花八门的，因为它是基于现有系统的集成，所以我们可以通过分析现有系统的 API 来确定我们能够创造出什么样的业务价值。例如很多 GPT 提供的联网功能就是通过集成 SerpAPI 来实现的，这是一个集成了各大搜索引擎的聚合查询网站，这样就能够让 ChatGPT 能够回答一些基于搜索的问题，如当前天气、股市、新闻等等。&lt;/p>
&lt;p>其中比较出名的自然是 &lt;a href="https://github.com/Significant-Gravitas/Auto-GPT">Auto GPT&lt;/a> 和 &lt;a href="https://github.com/reworkd/AgentGPT">AgentGPT&lt;/a> 这两个项目，如果对 GPT Agents 应用感兴趣，不妨看看这两个项目。除此之外，也许像 &lt;a href="https://github.com/calcom/cal.com/tree/main/apps/ai">cal.com&lt;/a> 这样的公司集成 AI agents 来使用自然语言增强预定会议这样的功能，也许能够给你一些不一样的启发。&lt;/p>
&lt;h3 id="非结构化输入和结构化输出">非结构化输入和结构化输出&lt;/h3>
&lt;p>还有就是我觉得被很多人忽视的一点是 GPT/LLM 处理非结构化数据的能力。在过去，我们想要处理一些简单的文本都是需要耗费大量开发时间的，例如从短信中提取出一些关键信息，例如姓名、电话号码、地址等等，由于不同的短信模版不同，所以这些信息都是非结构化。&lt;/p>
&lt;p>我们无法通过一个简单方法，就能够将这些内容转成 Json 等格式的结构化输出，所以我们往往需要通过一套复杂的正则表达式，或者 NLP 技术来将这些信息提取出来。但是正则表达式开发复杂，对于不同的短信模版，再加上模版会随着时间不短的改变，我们需要不断的调整正则表达式，这个过程是非常消耗开发时间和工程师精力的。&lt;/p>
&lt;p>NLP 技术又只能针对于某一些特定的场景，例如提取电话号码，提取地址等等，对于不同的场景，我们需要不同的 NLP 技术，所以一旦业务需求发生了变化，例如识别车牌，我们还需要重新开发和调整。&lt;/p>
&lt;p>但是有了 OpenAI GPT 这样统一的一个 API，我们只需要给服务提供不同的 Prompt，就可以通过 Prompt 来引导 GPT 进行推理，从而得到我们想要的结果。这样就能够大大的简化我们的开发流程，缩短开发时间，从而快速的响应市场的变化。&lt;/p>
&lt;p>并且由于 GPT 的强大的泛化能力，我们只需要针对于不同的场景，思考好如何处理非结构化数据即可。这种处理非结构化数据的能力，在未来软件开发中会改变很多以往业务的开发流程和方式，对于软件的生命周期将产生深远的影响。&lt;/p>
&lt;h3 id="自然语言交互">自然语言交互&lt;/h3>
&lt;p>最后我想说的是，GPT 能够处理自然语言的能力将深度改变人类与机器的交互方式。在过去，从命令行到图形界面，再到触摸屏，这些都是人机交互的变革史。大多数人和机器的交互方式其实是需要程序员这个载体的，人们有各种需求，业务分析师和开发人员将需求挖掘出来，由程序员通过代码的方式产生图形页面，人们再和图形页面产生交互，从而达到人机交互的能力。&lt;/p>
&lt;p>这个过程是信息传递损失的，并且产生了大量的限制和成本。如果机器能够理解自然语言，我们能够直接和机器进行交互，那么软件、机器、智能这几个我们熟悉的概念将会发生巨大的变化，人机交互也将发生翻天覆地的变化。&lt;/p>
&lt;p>如果你很难理解这种不确定的、没有任何参考的变化，可以去感受一下 &lt;a href="https://github.com/KillianLucas/open-interpreter">open interpreter&lt;/a> 这个项目，它是基于自然语言产生代码，并在电脑上直接执行的项目，虽然非常的原始和不稳定，但从中可以看到未来人机交互的变革。&lt;/p>
&lt;h2 id="references">References&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://github.com/guangzhengli/ChatFiles">https://github.com/guangzhengli/ChatFiles&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/guangzhengli/vectorhub">https://github.com/guangzhengli/vectorhub&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://js.langchain.com/docs/modules/data_connection">https://js.langchain.com/docs/modules/data_connection&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.datacamp.com/tutorial/open-ai-function-calling-tutorial">https://www.datacamp.com/tutorial/open-ai-function-calling-tutorial&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://openai.com/blog/function-calling-and-other-api-updates">https://openai.com/blog/function-calling-and-other-api-updates&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://openai.com/blog/chatgpt-plugins#code-interpreter">https://openai.com/blog/chatgpt-plugins#code-interpreter&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/KillianLucas/open-interpreter">https://github.com/KillianLucas/open-interpreter&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.chatpdf.com">https://www.chatpdf.com&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://jenni.ai">https://jenni.ai&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/reworkd/AgentGPT">https://github.com/reworkd/AgentGPT&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://a16z.com/how-are-consumers-using-generative-ai">https://a16z.com/how-are-consumers-using-generative-ai&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>向量数据库</title><link>https://guangzhengli.com/blog/zh/vector-database/</link><pubDate>Sat, 15 Jul 2023 15:15:00 +0800</pubDate><author>iguangzhengli@gmail.com (Guangzheng Li)</author><guid>https://guangzhengli.com/blog/zh/vector-database/</guid><description>&lt;p>也许你最近可能听过这样的新闻，某向量数据库的初创公司刚写好 PPT，就获得了几千万的投资，某公司的开源的向量数据库因其代码的简陋而登上了 Hackernews 等等。在过去几个月时间中， AI 应用的发展如火如荼，带动了 AI 应用技术栈上下游的火爆，而向量数据库就是其中最热门的之一。&lt;/p>
&lt;p>笔者最近因为开发 &lt;a href="https://github.com/guangzhengli/ChatFiles">ChatFiles&lt;/a> 和 &lt;a href="https://github.com/guangzhengli/vectorhub">VectorHub&lt;/a> 两款开源项目的需要从而对向量数据库（Vector Database）进行了学习，在对主流的向量数据库和搜索算法有了大概的了解后，笔者决定将这些知识整理成一篇文章，希望能够帮助到大家。&lt;/p>
&lt;h2 id="gpt-的缺陷">GPT 的缺陷&lt;/h2>
&lt;p>过去几个月的时间，我们正处于人工智能的革命中，其中最耀眼的莫过于 GPT-3.5/4 的横空出世，而 GPT-3.5/4 带给我们无限震撼的同时，其天然的缺陷和诸多的限制也让开发者头痛不已，例如其输入端上下文（tokens）大小的限制困扰着很多的开发者和消费者，像 gpt-3.5-turbo 模型它的限制是 4K tokens(～3000字)，这意味着使用者最多只能输入 3000 字给 GPT 来理解和推理答案。&lt;/p>
&lt;p>有人可能会疑惑，我使用的 ChatGPT 是有对话记忆功能的，既然它可以做到聊天记忆，那么它的输入端 token 有限制也没什么关系，只要我将给 ChatGPT 的文字内容拆分成多次输入，它自然就可以记住我之前的对话，从而做到解除 token 限制。&lt;/p>
&lt;p>这个想法是不太正确的，GPT 作为 LLM 模型是没有记忆功能的，所谓的记忆功能只是开发者将对话记录存储在内存或者数据库中，当你发送消息给 gpt 模型时，程序会自动将最近的几次对话记录（基于对话的字数限制在 4096 tokens 内）通过 prompt 组合成最终的问题，并发送给 ChatGPT。简而言之，如果你的对话记忆超过了 4096 tokens，那么它就会忘记之前的对话，这就是目前 GPT 在需求比较复杂的任务中无法克服的缺陷。&lt;/p>
&lt;p>目前，不同模型对于 token 的限制也不同，gpt-4 是 32K tokens 的限制，而目前最大的 token 限制是 &lt;a href="https://www.anthropic.com/index/100k-context-windows">Claude 模型的 100K&lt;/a>，这意味可以输入大约 75000 字的上下文给 GPT，这也意味着 GPT 直接理解一部《哈利波特》的所有内容并回答相关问题。&lt;/p>
&lt;p>但这样就能解决我们所有的问题了吗？答案是否定的，首先 Claude 给出的例子是 GPT 处理 72K tokens 上下文的响应速度是 22 秒。如果我们拥有 GB 级别或更大的文档需要进行 GPT 理解和问答，目前的算力很难带来良好体验，更关键的是目前 GPT API 的价格是按照 tokens 来收费的，所以输入的上下文越多，其价格越按昂贵。&lt;/p>
&lt;p>这种情况有点类似于早期开发者面对内存只有几 MB 甚至几 KB 时期开发应用的窘境，一是‘内存’昂贵，二是‘内存’太小，所以在 GPT 模型在性能、成本、注意力机制等方面有重大革命性进展前，开发者不得不面对的绕过 GPT tokens 限制的难题。&lt;/p>
&lt;h2 id="向量数据库的崛起">向量数据库的崛起&lt;/h2>
&lt;p>在 GPT 模型的限制下，开发者们不得不寻找其他的解决方案，而向量数据库就是其中之一。向量数据库的核心思想是将文本转换成向量，然后将向量存储在数据库中，当用户输入问题时，将问题转换成向量，然后在数据库中搜索最相似的向量和上下文，最后将文本返回给用户。&lt;/p>
&lt;p>当我们有一份文档需要 GPT 处理时，例如这份文档是客服培训资料或者操作手册，我们可以先将这份文档的所有内容转化成向量（这个过程称之为 Vector Embedding），然后当用户提出相关问题时，我们将用户的搜索内容转换成向量，然后在数据库中搜索最相似的向量，匹配最相似的几个上下文，最后将上下文返回给 GPT。这样不仅可以大大减少 GPT 的计算量，从而提高响应速度，更重要的是降低成本，并绕过 GPT 的 tokens 限制。&lt;/p>
&lt;p>&lt;img src="https://storage.guangzhengli.com/images/Embedding.png" alt="Embedding">&lt;/p>
&lt;p>再比如我们和 ChatGPT 之间有一份很长的对话，我们可以将所有对话以向量的方式保存起来，当我们提问给 ChatGPT 时，我们可以将问题转化为向量对过去所有的聊天记录进行语义搜索，找到与当前问题最相关的‘记忆’，一起发送给 ChatGPT，极大的提高 GPT 的输出质量。&lt;/p>
&lt;p>向量数据库的作用当然不止步于文字语义搜索，在传统的 AI 和机器学习场景中，还包含人脸识别、图像搜索、语音识别等功能，但不可否认的是，这一轮向量数据库的火爆，正是因为它对于 AI 获得理解和维护长期记忆以执行复杂任务时有非常大的帮助。例如你可以试试 &lt;a href="https://js.langchain.com/docs/">LangChainJs 的文档搜索/Q&amp;amp;A 功能&lt;/a> 感受它的魅力，或者可以试试笔者的开源项目 &lt;a href="https://github.com/guangzhengli/vectorhub">VectorHub&lt;/a> 和 &lt;a href="https://github.com/guangzhengli/ChatFiles">ChatFiles&lt;/a>，可以上传一份文档或者基于一份网页文档，然后询问文档相关问题。这些功能都是基于 Vector Embedding 和向量数据库的产品。&lt;/p>
&lt;h2 id="vector-embeddings">Vector Embeddings&lt;/h2>
&lt;p>对于传统数据库，搜索功能都是基于不同的索引方式（B Tree、倒排索引等）加上精确匹配和排序算法（BM25、TF-IDF）等实现的。本质还是基于文本的精确匹配，这种索引和搜索算法对于关键字的搜索功能非常合适，但对于语义搜索功能就非常弱。&lt;/p>
&lt;p>例如，如果你搜索“小狗”，那么你只能得到带有“小狗”关键字相关的结果，而无法得到“柯基”、“金毛”等结果，因为“小狗”和“金毛”是不同的词，传统数据库无法识别它们的语义关系，所以传统的应用需要人为的将“小狗”和“金毛”等词之间打上特征标签进行关联，这样才能实现语义搜索。而如何将生成和挑选特征这个过程，也被称为 Feature Engineering (特征工程)，它是将原始数据转化成更好的表达问题本质的特征的过程。&lt;/p>
&lt;p>但是如果你需要处理非结构化的数据，就会发现非结构化数据的特征数量会开始快速膨胀，例如我们处理的是图像、音频、视频等数据，这个过程就变得非常困难。例如，对于图像，可以标注颜色、形状、纹理、边缘、对象、场景等特征，但是这些特征太多了，而且很难人为的进行标注，所以我们需要一种自动化的方式来提取这些特征，而这可以通过 Vector Embedding 实现。&lt;/p>
&lt;p>Vector Embedding 是由 AI 模型（例如大型语言模型 LLM）生成的，它会根据不同的算法生成高维度的向量数据，代表着数据的不同特征，这些特征代表了数据的不同维度。例如，对于文本，这些特征可能包括词汇、语法、语义、情感、情绪、主题、上下文等。对于音频，这些特征可能包括音调、节奏、音高、音色、音量、语音、音乐等。&lt;/p>
&lt;p>例如对于目前来说，文本向量可以通过 OpenAI 的 text-embedding-ada-002 模型生成，图像向量可以通过 clip-vit-base-patch32 模型生成，而音频向量可以通过 wav2vec2-base-960h 模型生成。这些向量都是通过 AI 模型生成的，所以它们都是具有语义信息的。&lt;/p>
&lt;p>例如我们将这句话 “Your text string goes here” 用 text-embedding-ada-002 模型进行文本 Embedding，它会生成一个 1536 维的向量，得到的结果是这样：&lt;code>“-0.006929283495992422, -0.005336422007530928, ... -4547132266452536e-05,-0.024047505110502243”&lt;/code>，它是一个长度为 1536 的数组。这个向量就包含了这句话的所有特征，这些特征包括词汇、语法，我们可以将它存入向量数据库中，以便我们后续进行语义搜索。&lt;/p>
&lt;h2 id="特征和向量">特征和向量&lt;/h2>
&lt;p>虽然向量数据库的核心在于相似性搜索(Similarity Search)，但在深入了解相似性搜索前，我们需要先详细了解一下特征和向量的概念和原理。&lt;/p>
&lt;p>我们先思考一个问题？为什么我们在生活中区分不同的物品和事物？&lt;/p>
&lt;p>如果从理论角度出发，这是因为我们会通过识别不同事物之间不同的特征来识别种类，例如分别不同种类的小狗，就可以通过体型大小、毛发长度、鼻子长短等特征来区分。如下面这张照片按照体型排序，可以看到体型越大的狗越靠近坐标轴右边，这样就能得到一个体型特征的一维坐标和对应的数值，从 0 到 1 的数字中得到每只狗在坐标系中的位置。&lt;/p>
&lt;p>&lt;img src="https://storage.guangzhengli.com/images/Snipaste_2023-07-15_20-55-09.png" alt="Snipaste_2023-07-15_20-55-09">&lt;/p>
&lt;p>然而单靠一个体型大小的特征并不够，像照片中哈士奇、金毛和拉布拉多的体型就非常接近，我们无法区分。所以我们会继续观察其它的特征，例如毛发的长短。&lt;/p>
&lt;p>&lt;img src="https://storage.guangzhengli.com/images/Snipaste_2023-07-15_20-59-13.png" alt="Snipaste_2023-07-15_20-59-13">&lt;/p>
&lt;p>这样每只狗对应一个二维坐标点，我们就能轻易的将哈士奇、金毛和拉布拉多区分开来，如果这时仍然无法很好的区分德牧和罗威纳犬。我们就可以继续再从其它的特征区分，比如鼻子的长短，这样就能得到一个三维的坐标系和每只狗在三维坐标系中的位置。&lt;/p>
&lt;p>在这种情况下，只要特征足够多，就能够将所有的狗区分开来，最后就能得到一个高维的坐标系，虽然我们想象不出高维坐标系长什么样，但是在数组中，我们只需要一直向数组中追加数字就可以了。&lt;/p>
&lt;p>实际上，只要维度够多，我们就能够将所有的事物区分开来，世间万物都可以用一个多维坐标系来表示，它们都在一个高维的特征空间中对应着一个坐标点。&lt;/p>
&lt;p>那这和相似性搜索 (Similarity Search) 有什么关系呢？你会发现在上面的二维坐标中，德牧和罗威纳犬的坐标就非常接近，这就意味着它们的特征也非常接近。我们都知道向量是具有大小和方向的数学结构，所以可以将这些特征用向量来表示，这样就能够通过计算向量之间的距离来判断它们的相似度，这就是&lt;strong>相似性搜索&lt;/strong>。&lt;/p>
&lt;p>上面这几张图片和详细解释来源于&lt;a href="https://www.bilibili.com/video/BV11a4y1c7SW">这个视频&lt;/a>，这个视频系列也包含了部分下方介绍的相似性搜索算法。如果你对这个向量数据库感兴趣，非常推荐看看这个视频。&lt;/p>
&lt;h2 id="相似性搜索-similarity-search">相似性搜索 (Similarity Search)&lt;/h2>
&lt;p>既然我们知道了可以通过比较向量之间的距离来判断它们的相似度，那么如何将它应用到真实的场景中呢？如果想要在一个海量的数据中找到和某个向量最相似的向量，我们需要对数据库中的每个向量进行一次比较计算，但这样的计算量是非常巨大的，所以我们需要一种高效的算法来解决这个问题。&lt;/p>
&lt;p>高效的搜索算法有很多，其主要思想是通过两种方式提高搜索效率：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>减少向量大小——通过降维或减少表示向量值的长度。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>缩小搜索范围——可以通过聚类或将向量组织成基于树形、图形结构来实现，并限制搜索范围仅在最接近的簇中进行，或者通过最相似的分支进行过滤。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>我们首先来介绍一下大部分算法共有的核心概念，也就是聚类。&lt;/p>
&lt;h3 id="k-means-和-faiss">K-Means 和 Faiss&lt;/h3>
&lt;p>我们可以在保存向量数据后，先对向量数据先进行聚类。例如下图在二维坐标系中，划定了 4 个聚类中心，然后将每个向量分配到最近的聚类中心，经过聚类算法不断调整聚类中心位置，这样就可以将向量数据分成 4 个簇。每次搜索时，只需要先判断搜索向量属于哪个簇，然后再在这一个簇中进行搜索，这样就从 4 个簇的搜索范围减少到了 1 个簇，大大减少了搜索的范围。&lt;/p>
&lt;p>&lt;img src="https://storage.guangzhengli.com/images/kmeans.png" alt="kmeans">&lt;/p>
&lt;p>常见的聚类算法有 K-Means，它可以将数据分成 k 个类别，其中 k 是预先指定的。以下是 k-means 算法的基本步骤：&lt;/p>
&lt;ol>
&lt;li>选择 k 个初始聚类中心。&lt;/li>
&lt;li>将每个数据点分配到最近的聚类中心。&lt;/li>
&lt;li>计算每个聚类的新中心。&lt;/li>
&lt;li>重复步骤 2 和 3，直到聚类中心不再改变或达到最大迭代次数。&lt;/li>
&lt;/ol>
&lt;p>但是这种搜索方式也有一些缺点，例如在搜索的时候，如果搜索的内容正好处于两个分类区域的中间，就很有可能遗漏掉最相似的向量。&lt;/p>
&lt;p>现实情况中，向量的分布也不会像图中一样区分的那么明显，往往区域的边界是相邻的，就像下图 &lt;a href="https://github.com/facebookresearch/faiss">Faiss 算法&lt;/a> 一样。&lt;/p>
&lt;p>我们可以将向量想象为包含在 Voronoi 单元格中 - 当引入一个新的查询向量时，首先测量其与质心 (centroids) 之间的距离，然后将搜索范围限制在该质心所在的单元格内。&lt;/p>
&lt;p>&lt;img src="https://storage.guangzhengli.com/images/WUjl5M.jpg" alt="WUjl5M">&lt;/p>
&lt;p>那么为了解决搜索时可能存在的遗漏问题，可以将搜索范围动态调整，例如当 nprobe = 1 时，只搜索最近的一个聚类中心，当 nprobe = 2 时，搜索最近的两个聚类中心，根据实际业务的需求调整 nprobe 的值。&lt;/p>
&lt;p>&lt;img src="https://storage.guangzhengli.com/images/FZadSG.jpg" alt="FZadSG">&lt;/p>
&lt;p>实际上，除了暴力搜索能完美的搜索出最相邻，所有的搜索算法只能在速度和质量还有内存上做一个权衡，这些算法也被称为近似最相邻（Approximate Nearest Neighbor）。&lt;/p>
&lt;h3 id="product-quantization-pq">Product Quantization (PQ)&lt;/h3>
&lt;p>在大规模数据集中，聚类算法最大的问题在于内存占用太大。这主要体现在两个方面，首先因为需要保存每个向量的坐标，而每个坐标都是一个浮点数，占用的内存就已经非常大了。除此之外，还需要维护聚类中心和每个向量的聚类中心索引，这也会占用大量的内存。&lt;/p>
&lt;p>对于第一个问题，可以通过量化 (Quantization) 的方式解决，也就是常见的有损压缩。例如在内存中可以将聚类中心里面每一个向量都用聚类中心的向量来表示，并维护一个所有向量到聚类中心的码本，这样就能大大减少内存的占用。&lt;/p>
&lt;p>但这仍然不能解决所有问题，在前面一个例子中，在二维坐标系中划分了聚类中心，同理，在高维坐标系中，也可以划定多个聚类中心点，不断调整和迭代，直到找到多个稳定和收敛的中心点。&lt;/p>
&lt;p>但是在高维坐标系中，还会遇到维度灾难问题，具体来说，随着维度的增加，数据点之间的距离会呈指数级增长，这也就意味着，在高维坐标系中，需要更多的聚类中心点将数据点分成更小的簇，才能提高分类的质量。否者，向量和自己的聚类中心距离很远，会极大的降低搜索的速度和质量。&lt;/p>
&lt;p>但如果想要维持分类和搜索质量，就需要维护数量庞大的聚类中心。随之而来会带来另一个问题，那就是聚类中心点的数量会随着维度的增加而指数级增长，这样会导致我们存储码本的数量极速增加，从而极大的增加了内存的消耗。例如一个 128 维的向量，需要维护 2^64 个聚类中心才能维持不错的量化结果，但这样的码本存储大小已经超过维护原始向量的内存大小了。&lt;/p>
&lt;p>解决这个问题的方法是将向量分解为多个子向量，然后对每个子向量独立进行量化，比如将 128 维的向量分为 8 个 16 维的向量，然后在 8 个 16 维的子向量上分别进行聚类，因为 16 维的子向量大概只需要 256 个聚类中心就能得到还不错的量化结果，所以就可以将码本的大小从 2^64 降低到 8 * 256 = 2048 个聚类中心，从而降低内存开销。&lt;/p>
&lt;p>&lt;img src="https://storage.guangzhengli.com/images/5dAeV5.jpg" alt="5dAeV5">&lt;/p>
&lt;p>而将向量进行编码后，也将得到 8 个编码值，将它们拼起来就是该向量的最终编码值。等到使用的时候，只需要将这 8 个编码值，然后分别在 8 个子码本中搜索出对应的 16 维的向量，就能将它们使用笛卡尔积的方式组合成一个 128 维的向量，从而得到最终的搜索结果。这也就是乘积量化（Product Quantization）的原理。&lt;/p>
&lt;p>使用 PQ 算法，可以显著的减少内存的开销，同时加快搜索的速度，它唯一的问题是搜索的质量会有所下降，但就像我们刚才所讲，所有算法都是在内存、速度和质量上做一个权衡。&lt;/p>
&lt;h3 id="hierarchical-navigable-small-worlds-hnsw">Hierarchical Navigable Small Worlds (HNSW)&lt;/h3>
&lt;p>除了聚类以外，也可以通过构建树或者构建图的方式来实现近似最近邻搜索。这种方法的基本思想是每次将向量加到数据库中的时候，就先找到与它最相邻的向量，然后将它们连接起来，这样就构成了一个图。当需要搜索的时候，就可以从图中的某个节点开始，不断的进行最相邻搜索和最短路径计算，直到找到最相似的向量。&lt;/p>
&lt;p>这种算法能保证搜索的质量，但是如果图中所以的节点都以最短的路径相连，如图中最下面的一层，那么在搜索的时候，就同样需要遍历所有的节点。&lt;/p>
&lt;p>&lt;img src="https://storage.guangzhengli.com/images/GD7ufK.jpg" alt="GD7ufK">&lt;/p>
&lt;p>解决这个问题的思路与常见的跳表算法相似，如下图要搜索跳表，从最高层开始，沿着具有最长“跳过”的边向右移动。如果发现当前节点的值大于要搜索的值-我们知道已经超过了目标，因此我们会在下一级中向前一个节点。&lt;/p>
&lt;p>&lt;img src="https://storage.guangzhengli.com/images/wOu6JL.jpg" alt="wOu6JL">&lt;/p>
&lt;p>HNSW 继承了相同的分层格式，最高层具有更长的边缘（用于快速搜索），而较低层具有较短的边缘（用于准确搜索）。&lt;/p>
&lt;p>具体来说，可以将图分为多层，每一层都是一个小世界，图中的节点都是相互连接的。而且每一层的节点都会连接到上一层的节点，当需要搜索的时候，就可以从第一层开始，因为第一层的节点之间距离很长，可以减少搜索的时间，然后再逐层向下搜索，又因为最下层相似节点之间相互关联，所以可以保证搜索的质量，能够找到最相似的向量。&lt;/p>
&lt;p>如果你对跳表和 HNSW 感兴趣，可以看看&lt;a href="https://www.youtube.com/watch?v=QvKMwLjdK-s&amp;amp;t=168s&amp;amp;ab_channel=JamesBriggs">这个视频&lt;/a>。&lt;/p>
&lt;p>HNSW 算法是一种经典的空间换时间的算法，它的搜索质量和搜索速度都比较高，但是它的内存开销也比较大，因为不仅需要将所有的向量都存储在内存中。还需要维护一个图的结构，也同样需要存储。所以这类算法需要根据实际的场景来选择。&lt;/p>
&lt;h3 id="locality-sensitive-hashing-lsh">Locality Sensitive Hashing (LSH)&lt;/h3>
&lt;p>局部敏感哈希（Locality Sensitive Hashing）也是一种使用近似最近邻搜索的索引技术。它的特点是快速，同时仍然提供一个近似、非穷举的结果。LSH 使用一组哈希函数将相似向量映射到“桶”中，从而使相似向量具有相同的哈希值。这样，就可以通过比较哈希值来判断向量之间的相似度。&lt;/p>
&lt;p>通常，我们设计的哈希算法都是力求减少哈希碰撞的次数，因为哈希函数的搜索时间复杂度是 O(1)，但是，如果存在哈希碰撞，即两个不同的关键字被映射到同一个桶中，那么就需要使用链表等数据结构来解决冲突。在这种情况下，搜索的时间复杂度通常是 O(n)，其中n是链表的长度。所以为了提高哈希函数的搜索的效率，通常会将哈希函数的碰撞概率尽可能的小。&lt;/p>
&lt;p>但是在向量搜索中，我们的目的是为了找到相似的向量，所以可以专门设计一种哈希函数，使得哈希碰撞的概率尽可能高，并且位置越近或者越相似的向量越容易碰撞，这样相似的向量就会被映射到同一个桶中。&lt;/p>
&lt;p>等搜索特定向量时，为了找到给定查询向量的最近邻居，使用相同的哈希函数将类似向量“分桶”到哈希表中。查询向量被散列到特定表中，然后与该表中的其他向量进行比较以找到最接近的匹配项。这种方法比搜索整个数据集要快得多，因为每个哈希表桶中的向量远少于整个空间中的向量数。&lt;/p>
&lt;p>那么这个哈希函数应该如何设计呢？为了大家更好理解，我们先从二维坐标系解释，如下所图示，在二维坐标系中可以通过随机生成一条直线，将二维坐标系划分为两个区域，这样就可以通过判断向量是否在直线的同一边来判断它们是否相似。例如下图通过随机生成 4 条直线，这样就可以通过 4 个二进制数来表示一个向量的位置，例如 A 和 B 表示向量在同一个区域。&lt;/p>
&lt;p>&lt;img src="https://storage.guangzhengli.com/images/lsh2.png" alt="lsh2">&lt;/p>
&lt;p>这个原理很简单，如果两个向量的距离很近，那么它们在直线的同一边的概率就会很高，例如直线穿过 AC 的概率就远大于直线穿过 AB 的概率。所以 AB 在同一侧的概率就远大于 AC 在同一侧的概率。&lt;/p>
&lt;p>当搜索一个向量时，将这个向量再次进行哈希函数计算，得到相同桶中的向量，然后再通过暴力搜索的方式，找到最接近的向量。如下图如果再搜索一个向量经过了哈希函数，得到了 10 的值，就会直接找到和它同一个桶中相似的向量 D。从而大大减少了搜索的时间。&lt;/p>
&lt;p>&lt;img src="https://storage.guangzhengli.com/images/lsh3.png" alt="lsh3">&lt;/p>
&lt;p>关于更多 LSH 算法的细节，可以参考&lt;a href="https://www.pinecone.io/learn/series/faiss/locality-sensitive-hashing/">这篇博客&lt;/a>。&lt;/p>
&lt;h4 id="random-projection-for-lsh-随机投影">Random Projection for LSH 随机投影&lt;/h4>
&lt;p>如果在二维坐标系可以通过随机生成的直线区分相似性，那么同理，在三维坐标系中，就可以通过随机生成一个平面，将三维坐标系划分为两个区域。在多维坐标系中，同样可以通过随机生成一个超平面，将多维坐标系划分为两个区域，从而区分相似性。&lt;/p>
&lt;p>但是在高维空间中，数据点之间的距离往往非常稀疏，数据点之间的距离会随着维度的增加呈指数级增长。导致计算出来的桶非常多，最极端的情况是每个桶中就一个向量，并且计算速度非常慢。所以实际上在实现 LSH 算法的时候，会考虑使用随机投影的方式，将高维空间的数据点投影到低维空间，从而减少计算的时间和提高查询的质量。&lt;/p>
&lt;p>随机投影背后的基本思想是使用随机投影矩阵将高维向量投影到低维空间中。创建一个由随机数构成的矩阵，其大小将是所需的目标低维值。然后，计算输入向量和矩阵之间的点积，得到一个被投影的矩阵，它比原始向量具有更少的维度但仍保留了它们之间的相似性。&lt;/p>
&lt;p>当我们查询时，使用相同的投影矩阵将查询向量投影到低维空间。然后，将投影的查询向量与数据库中的投影向量进行比较，以找到最近邻居。由于数据的维数降低了，搜索过程比在整个高维空间中搜索要快得多。&lt;/p>
&lt;p>其基本步骤是：&lt;/p>
&lt;ol>
&lt;li>从高维空间中随机选择一个超平面，将数据点投影到该超平面上。&lt;/li>
&lt;li>重复步骤 1，选择多个超平面，将数据点投影到多个超平面上。&lt;/li>
&lt;li>将多个超平面的投影结果组合成一个向量，作为低维空间中的表示。&lt;/li>
&lt;li>使用哈希函数将低维空间中的向量映射到哈希桶中。&lt;/li>
&lt;/ol>
&lt;p>同样，随机投影也是一种近似方法，并且投影质量取决于投影矩阵。通常情况下，随机性越大的投影矩阵，其映射质量就越好。但是生成真正随机的投影矩阵可能会计算成本很高，特别是对于大型数据集来说。关于更多 RP for LSH 算法的细节，可以参考&lt;a href="https://www.pinecone.io/learn/series/faiss/locality-sensitive-hashing-random-projection/">这篇博客&lt;/a>。&lt;/p>
&lt;h2 id="相似性测量-similarity-measurement">相似性测量 (Similarity Measurement)&lt;/h2>
&lt;p>上面我们讨论了向量数据库的不同搜索算法，但是还没有讨论如何衡量相似性。在相似性搜索中，需要计算两个向量之间的距离，然后根据距离来判断它们的相似度。&lt;/p>
&lt;p>而如何计算向量在高维空间的距离呢？有三种常见的向量相似度算法：欧几里德距离、余弦相似度和点积相似度。&lt;/p>
&lt;h3 id="欧几里得距离euclidean-distance">欧几里得距离（Euclidean Distance）&lt;/h3>
&lt;p>欧几里得距离是指两个向量之间的距离，它的计算公式为：&lt;/p>
&lt;p>$$d(\mathbf{A}, \mathbf{B}) = \sqrt{\sum_{i=1}^{n}(A_i - B_i)^2}$$&lt;/p>
&lt;p>其中，$\mathbf{A}$ 和 $\mathbf{B}$ 分别表示两个向量，$n$ 表示向量的维度。&lt;/p>
&lt;p>&lt;img src="https://storage.guangzhengli.com/images/KPhJ27.jpg" alt="KPhJ27">&lt;/p>
&lt;p>欧几里得距离算法的优点是可以反映向量的绝对距离，适用于需要考虑向量长度的相似性计算。例如推荐系统中，需要根据用户的历史行为来推荐相似的商品，这时就需要考虑用户的历史行为的数量，而不仅仅是用户的历史行为的相似度。&lt;/p>
&lt;h3 id="余弦相似度cosine-similarity">余弦相似度（Cosine Similarity）&lt;/h3>
&lt;p>余弦相似度是指两个向量之间的夹角余弦值，它的计算公式为：&lt;/p>
&lt;p>$$\cos(\theta) = \frac{\mathbf{A} \cdot \mathbf{B}}{|\mathbf{A}| |\mathbf{B}|}$$&lt;/p>
&lt;p>其中，$\mathbf{A}$ 和 $\mathbf{B}$ 分别表示两个向量，$\cdot$ 表示向量的点积，$|\mathbf{A}|$ 和 $|\mathbf{B}|$ 分别表示两个向量的模长。&lt;/p>
&lt;p>&lt;img src="https://storage.guangzhengli.com/images/fHLAfz.jpg" alt="fHLAfz">&lt;/p>
&lt;p>余弦相似度对向量的长度不敏感，只关注向量的方向，因此适用于高维向量的相似性计算。例如语义搜索和文档分类。&lt;/p>
&lt;h3 id="点积相似度-dot-product-similarity">点积相似度 (Dot product Similarity)&lt;/h3>
&lt;p>向量的点积相似度是指两个向量之间的点积值，它的计算公式为：&lt;/p>
&lt;p>$$\mathbf{A} \cdot \mathbf{B} = \sum_{i=1}^{n}A_i B_i$$&lt;/p>
&lt;p>其中，$\mathbf{A}$ 和 $\mathbf{B}$ 分别表示两个向量，$n$ 表示向量的维度。&lt;/p>
&lt;p>&lt;img src="https://storage.guangzhengli.com/images/kyA3AN.jpg" alt="kyA3AN">&lt;/p>
&lt;p>点积相似度算法的优点在于它简单易懂，计算速度快，并且兼顾了向量的长度和方向。它适用于许多实际场景，例如图像识别、语义搜索和文档分类等。但点积相似度算法对向量的长度敏感，因此在计算高维向量的相似性时可能会出现问题。&lt;/p>
&lt;p>每一种相似性测量 (Similarity Measurement) 算法都有其优点和缺点，需要开发者根据自己的数据特征和业务场景来选择。&lt;/p>
&lt;h2 id="过滤-filtering">过滤 (Filtering)&lt;/h2>
&lt;p>在实际的业务场景中，往往不需要在整个向量数据库中进行相似性搜索，而是通过部分的业务字段进行过滤再进行查询。所以存储在数据库的向量往往还需要包含元数据，例如用户 ID、文档 ID 等信息。这样就可以在搜索的时候，根据元数据来过滤搜索结果，从而得到最终的结果。&lt;/p>
&lt;p>为此，向量数据库通常维护两个索引：一个是向量索引，另一个是元数据索引。然后，在进行相似性搜索本身之前或之后执行元数据过滤，但无论哪种情况下，都存在导致查询过程变慢的困难。&lt;/p>
&lt;p>&lt;img src="https://storage.guangzhengli.com/images/VwZxFW.jpg" alt="VwZxFW">&lt;/p>
&lt;p>过滤过程可以在向量搜索本身之前或之后执行，但每种方法都有自己的挑战，可能会影响查询性能：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Pre-filtering：在向量搜索之前进行元数据过滤。虽然这可以帮助减少搜索空间，但也可能导致系统忽略与元数据筛选标准不匹配的相关结果。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Post-filtering：在向量搜索完成后进行元数据过滤。这可以确保考虑所有相关结果，在搜索完成后将不相关的结果进行筛选。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>为了优化过滤流程，向量数据库使用各种技术，例如利用先进的索引方法来处理元数据或使用并行处理来加速过滤任务。平衡搜索性能和筛选精度之间的权衡对于提供高效且相关的向量数据库查询结果至关重要。&lt;/p>
&lt;h2 id="向量数据库选型">向量数据库选型&lt;/h2>
&lt;p>笔者在本文中，花费了大量的笔墨来介绍向量数据库的相似性搜索算法的原理和实现，相似性搜索算法固然是一个向量数据库的核心和关键点，但是在实际的业务场景中，往往还需要考虑其它的因素，例如向量数据库的可用性、扩展性、安全性等，还有代码是否开源、社区是否活跃等等。&lt;/p>
&lt;h3 id="分布式">分布式&lt;/h3>
&lt;p>一个成熟的向量数据库，往往需要支持分布式部署，这样才能满足大规模数据的存储和查询。数据拥有的越多，需要节点就越多，出现的错误和故障也就越多，所以分布式的向量数据库需要具备高可用性和容错性。&lt;/p>
&lt;p>数据库的高可用性和容错性，往往需要实现分片和复制能力，在传统的数据库中，往往通过数据的主键或者根据业务需求进行分片，但是在分布式的向量数据库中，就需要考虑根据向量的相似性进行分区，以便查询的时候能够保证结果的质量和速度。&lt;/p>
&lt;p>其它类似复制节点数据的一致性、数据的安全性等等，都是分布式向量数据库需要考虑的因素。&lt;/p>
&lt;h3 id="访问控制和备份">访问控制和备份&lt;/h3>
&lt;p>除此之外，访问控制设计的是否充足，例如当组织和业务快速发展时，是否能够快速的添加新的用户和权限控制，是否能够快速的添加新的节点，审计日志是否完善等等，都是需要考虑的因素。&lt;/p>
&lt;p>另外，数据库的监控和备份也是一个重要的因素，当数据出现故障时，能够快速的定位问题和恢复数据，是一个成熟的向量数据库必须要考虑的因素。&lt;/p>
&lt;h3 id="api--sdk">API &amp;amp; SDK&lt;/h3>
&lt;p>对比上面的因素选择，API &amp;amp; SDK 可能是往往被忽略的因素，但是在实际的业务场景中，API &amp;amp; SDK 往往是开发者最关心的因素。因为 API &amp;amp; SDK 的设计直接影响了开发者的开发效率和使用体验。一个优秀良好的 API &amp;amp; SDK 设计，往往能够适应需求的不同变化，向量数据库是一个新的领域，在如今大部分人不太清楚这方面需求的当下，这一点容易被人忽视。&lt;/p>
&lt;h3 id="选型">选型&lt;/h3>
&lt;p>截至目前，汇总到目前的向量数据库有以下几种选择：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>向量数据库&lt;/th>
&lt;th>URL&lt;/th>
&lt;th>GitHub Star&lt;/th>
&lt;th>Language&lt;/th>
&lt;th>Cloud&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>chroma&lt;/td>
&lt;td>&lt;a href="https://github.com/chroma-core/chroma">https://github.com/chroma-core/chroma&lt;/a>&lt;/td>
&lt;td>7.4K&lt;/td>
&lt;td>Python&lt;/td>
&lt;td>❌&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>milvus&lt;/td>
&lt;td>&lt;a href="https://github.com/milvus-io/milvus">https://github.com/milvus-io/milvus&lt;/a>&lt;/td>
&lt;td>21.5K&lt;/td>
&lt;td>Go/Python/C++&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>pinecone&lt;/td>
&lt;td>&lt;a href="https://www.pinecone.io/">https://www.pinecone.io/&lt;/a>&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>qdrant&lt;/td>
&lt;td>&lt;a href="https://github.com/qdrant/qdrant">https://github.com/qdrant/qdrant&lt;/a>&lt;/td>
&lt;td>11.8K&lt;/td>
&lt;td>Rust&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>typesense&lt;/td>
&lt;td>&lt;a href="https://github.com/typesense/typesense">https://github.com/typesense/typesense&lt;/a>&lt;/td>
&lt;td>12.9K&lt;/td>
&lt;td>C++&lt;/td>
&lt;td>❌&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>weaviate&lt;/td>
&lt;td>&lt;a href="https://github.com/weaviate/weaviate">https://github.com/weaviate/weaviate&lt;/a>&lt;/td>
&lt;td>6.9K&lt;/td>
&lt;td>Go&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="传统数据的扩展">传统数据的扩展&lt;/h3>
&lt;p>除了选择专业的向量数据库，使用传统数据库进行扩展也是一种方法。类似 Redis 除了传统的 Key Value 数据库用途外，Redis 还提供了 Redis Modules，这是一种通过新功能、命令和数据类型扩展 Redis 的方式。例如使用 &lt;a href="https://redis.io/docs/interact/search-and-query/">RediSearch&lt;/a> 模块来扩展向量搜索的功能。&lt;/p>
&lt;p>同理的还有 PostgreSQL 的扩展，PostgreSQL 提供使用 extension 的方式来扩展数据库的功能，例如 &lt;a href="https://github.com/pgvector/pgvector">pgvector&lt;/a> 来开启向量搜索的功能。它不仅支持精确和相似性搜索，还支持余弦相似度等相似性测量算法。最重要的是，它是附加在 PostgreSQL 上的，因此可以利用 PostgreSQL 的所有功能，例如 ACID 事务、并发控制、备份和恢复等。还拥有所有的 PostgreSQL 客户端库，因此可以使用任何语言的 PostgreSQL 客户端来访问它。可以减少开发者的学习成本和服务的维护成本。&lt;/p>
&lt;p>像笔者的开源项目 &lt;a href="https://github.com/guangzhengli/ChatFiles">ChatFiles&lt;/a> 和 &lt;a href="https://github.com/guangzhengli/vectorhub">VectorHub&lt;/a> 目前就暂时使用 pgvector 来实现向量搜索以实现 GPT 文档问答，基于 &lt;a href="https://supabase.com/blog/openai-embeddings-postgres-vector">Supabase 提供的 PostgreSQL + pgvector&lt;/a> 服务完成。&lt;/p>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>本文主要介绍了向量数据库的原理和实现，包括向量数据库的基本概念、相似性搜索算法、相似性测量算法、过滤算法和向量数据库的选型等等。向量数据库是崭新的领域，目前大部分向量数据库公司的估值乘着 AI 和 GPT 的东风从而飞速的增长，但是在实际的业务场景中，目前向量数据库的应用场景还比较少，抛开浮躁的外衣，向量数据库的应用场景还需要开发者们和业务专家们去挖掘。&lt;/p>
&lt;h2 id="references">References&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.bilibili.com/video/BV11a4y1c7SW">https://www.bilibili.com/video/BV11a4y1c7SW&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.bilibili.com/video/BV1BM4y177Dk">https://www.bilibili.com/video/BV1BM4y177Dk&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.pinecone.io/learn/vector-database/">https://www.pinecone.io/learn/vector-database/&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/guangzhengli/ChatFiles">https://github.com/guangzhengli/ChatFiles&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/guangzhengli/vectorhub">https://github.com/guangzhengli/vectorhub&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.anthropic.com/index/100k-context-windows">https://www.anthropic.com/index/100k-context-windows&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://js.langchain.com/docs/">https://js.langchain.com/docs/&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.pinecone.io/learn/series/faiss/locality-sensitive-hashing/">https://www.pinecone.io/learn/series/faiss/locality-sensitive-hashing/&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.pinecone.io/learn/series/faiss/product-quantization/">https://www.pinecone.io/learn/series/faiss/product-quantization/&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.pinecone.io/learn/series/faiss/locality-sensitive-hashing-random-projection/">https://www.pinecone.io/learn/series/faiss/locality-sensitive-hashing-random-projection/&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=QvKMwLjdK-s&amp;amp;t=168s&amp;amp;ab_channel=JamesBriggs">https://www.youtube.com/watch?v=QvKMwLjdK-s&amp;amp;t=168s&amp;amp;ab_channel=JamesBriggs&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.pinecone.io/learn/series/faiss/faiss-tutorial/">https://www.pinecone.io/learn/series/faiss/faiss-tutorial/&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=sKyvsdEv6rk&amp;amp;ab_channel=JamesBriggs">https://www.youtube.com/watch?v=sKyvsdEv6rk&amp;amp;ab_channel=JamesBriggs&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.pinecone.io/learn/vector-similarity/">https://www.pinecone.io/learn/vector-similarity/&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/chroma-core/chroma">https://github.com/chroma-core/chroma&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/milvus-io/milvus">https://github.com/milvus-io/milvus&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.pinecone.io/">https://www.pinecone.io/&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/qdrant/qdrant">https://github.com/qdrant/qdrant&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/typesense/typesense">https://github.com/typesense/typesense&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/weaviate/weaviate">https://github.com/weaviate/weaviate&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://redis.io/docs/interact/search-and-query/">https://redis.io/docs/interact/search-and-query/&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/pgvector/pgvector">https://github.com/pgvector/pgvector&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>