---
title: GPT 应用开发和思考
date: 2023-07-31T20:10:00+08:00
tags: ["llm", "gpt", "vector", "embeddings", "database", "chatbot"]
series: ["GPT Development"]
featured: true
math: true
---

在过去几个月的时间中，我们似乎正处于人工智能的革命中。除了大多数人了解的 OpenAI ChatGPT 之外，许多非常新颖、有趣、实用的 AI 应用也是层出不穷，并且在使用这些应用时时，笔者也确确实实的感受到了生产力的提高。

但是关于 GPT 应用的开发知识和路线，目前似乎还没有太多的资料，所以笔者决定将自己的一些经验和思考整理成一个系列，希望能够帮助到大家。

本篇文章主要介绍的是 GPT 相关应用的开发思考，在今年 4 月份的时候，笔者因为开发 [ChatFiles](https://github.com/guangzhengli/ChatFiles) 这个开源项目，从而学习了 GPT 相关的技术知识，但是由于笔者的时间精力有限，所以一直没有机会将这些知识整理成一篇文章，直到最近笔者又因为有了新的想法，开源了 [VectorHub](https://github.com/guangzhengli/vectorhub) 这个同样基于 GPT Prompt 和 Embeddings 技术的项目，进而对 GPT 和 Embeddings 等技术知识有了更深入的了解，所以就有了这一篇分享。

<!--more-->

## 从 Prompt 开始

AI 应用开发在过去一段时间内吸引了众多开发者入场，除了大家所熟知的 ChatGPT 之外，还涌现了大量有实际价值的 AI 应用，例如基于 AI 的翻译类的应用如 [openai-translator](https://github.com/openai-translator/openai-translator)、[immersivetranslate](https://immersivetranslate.com/)，写作类的应用如 [Notion AI](https://www.notion.so/product/ai)，编程辅助类的应用如 [GitHub Copilot](https://github.com/features/copilot) 和 [GitHub Copilot Chat](https://docs.github.com/en/copilot/github-copilot-chat/using-github-copilot-chat?tool=vscode)。

这些应用有些是优化了原有的体验，如基于 GPT 的翻译的 [openai-translator](https://github.com/openai-translator/openai-translator)，翻译质量和阅读体验远胜于之前的机器翻译，还有些则是提供了之前无法实现的功能，如 [GitHub Copilot](https://github.com/features/copilot) 的代码补全和生成，还有像 [GitHub Copilot Chat](https://docs.github.com/en/copilot/github-copilot-chat/using-github-copilot-chat?tool=vscode) 提供回答编码相关问题、解释代码、生成单元测试、给错误代码提出修复意见等等功能，这些功能的实现难度在以前是完全无法想象的。

这些应用在功能上虽然没有相似之处，但是在实现原理中，它们都是主要基于 GPT 的 Prompt(提示)实现。Prompt 指的是提供给模型的文本或指令，可以用来引导模型生成自然语言输出(Completion)。它可以给模型提供上下文信息，对模型的输出结果至关重要。

我们知道 GPT（Generative Pre-trained Transformer）是一个推理模型，它主要基于预训练和微调两个阶段。

在预训练阶段使用一个大规模的语料库，例如维基百科、新闻文章、小说等，当我们输入一个 Prompt 给它，它会基于这个 Prompt 给出一个预测的结果，这个结果是基于它在预训练阶段学习到的知识，并通过概率生成一个个的单词组合而来。这也是为什么相同的 Prompt 输入，每次的结果都会有所不同，因为每次的结果都是基于概率生成的，这也是它被称为生成式 AI 的原因。

在预训练完成后，在微调阶段会将 GPT 模型加载到特定的任务上，并使用该任务的数据集对模型进行训练。这样，模型就可以根据任务的要求进行微调，以便更好地理解 Prompt 并生成与任务相关的文本。通过微调，GPT 可以适应不同的任务，如文本分类、情感分析、问答系统等。

所以这也就能理解为什么 Prompt 对于 GPT 应用开发的重要性，因为它是除了微调以外，我们能与 GPT 模型唯一的交互方式（当然除此之外还可以通过调整模型的 temperature 和 top_p 两个配置来控制 GPT 更多样化或更具创造性的输出，不过对于输出质量和对下游任务的处理能力并无明显影响）。所以 Prompt 是 GPT 应用开发最核心的部分，也是最需要开发者去思考和优化的部分。

## Prompt 学习路线

关于 Prompt 的基础知识，可以先去看看吴恩达老师的 [ChatGPT Prompt 工程](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/)。可以通过两个小时不到的视频可以快速的了解 Prompt 的使用方式和它的魅力所在。

在有了一个初步的了解之后，笔者推荐 [Prompt Engineering Guide](https://www.promptingguide.ai/) 这份文档，该文档包含了大量的 Prompt 基础知识和未来的发展方向，对于 GPT 应用开发者来说，除了学习 Prompt 的基础知识之外，还可以从中获取到一些工程界和学术界对于 Prompt 的发展方向的思考，对于开发 AI 应用来说，这些思考弥足珍贵。

最后，非常推荐大家去看看 OpenAI 官方的 [GPT 最佳实践](https://platform.openai.com/docs/guides/gpt-best-practices) 这份文档，它是由 OpenAI 官方提供的 GPT 最佳实践指南，里面包含了大量的 Prompt 示例和使用技巧，对于 GPT 应用开发者来说，是一份非常有价值的文档。因为它是 OpenAI 官方通过合作伙伴或者 Hackathon 等不同的方式，在不同的业务领域 GPT 应用开发中总结出来的最佳实践，对于开发者来讲非常有启发价值！下面是对这一份文档的摘录：

{{< tweet user="iguangzhengli" id="1668059021390266369" >}}

## Prompt 最佳实践

关于 Prompt 编写的最佳实践，最为推荐的当然是 OpenAI 官方出品的 [GPT 最佳实践](https://platform.openai.com/docs/guides/gpt-best-practices) 这份文档，但对于开发 GPT 应用来讲，笔者还是想结合这份最佳实践和一些自己的经验，给大家分享一些 GPT 开发的实践。

### 清晰和详细

现实中大部分开发者在平常使用 GPT 的时候，都是以解决编程问题或者询问问题为主，所以容易带入以往使用 Google 等搜索引擎的经验来使用和开发 GPT。

例如当你想知道如何用 Python 编写斐波那契数列的时候，如果在以前使用 Google 搜索引擎，你可能会输入 `python fibonacci`。因为这样就足够了，Google 是基于倒排索引和 PageRank 算法的，只需要输入关键字，就能得到高质量的网页答案。

所以只需要这种输入两个字的输入方式是最简单和最高效的，毕竟就算多输入几个字 `how to write python fibonacci` ，对于 Google 搜索引擎来讲，输出质量是相差不大的。

而如果你使用的是 GPT，像 `python fibonacci` 这样的输入对于 GPT 来说是非常不友好的，因为它无法清晰的理解你的意图，所以它可能会给出一些不相关的结果（根据不同模型的质量会略有差异）。

但如果你输入的是 `编写一个用 python 函数来高效地计算 fibonacci 数列。评论每一行代码以解释每一部分的作用以及为什么这样写`。这样的输入对于 GPT 来说就非常清晰和详细，它能够清晰的理解你的意图，所以它给出的结果也会更加准确，**在保证输出质量下限的同时，还可以提高输出质量的上限！**

这和开发者以往使用 Google 等搜索引擎的经验是完全不同的，也是 GPT 开发者和使用者最容易忽视的地方。笔者在今年很早的时候开发 [ChatFiles](https://github.com/guangzhengli/ChatFiles) 项目时，曾匿名收集过使用者的 Prompt，发现 95% 以上的使用者使用的都是非常简单的 Prompt，简单的甚至看起来有一种一字千金的感觉。

所以当开发者开发 GPT 应用时，一定要注意 Prompt 的清晰和详细，多试几次，选择一个输出质量稳定和格式相同的 Prompt，这是保证 GPT 应用质量的关键。

### 如何处理更加复杂的任务

相信所有开发者面对简单的场景任务，多花费一些时间来调整 Prompt，都能设计好程序的 Prompt 并得到不错的输出质量。但是面对复杂任务时，想要提高 GPT 的输出质量还需要两个非常重要的技巧：**让 GPT 推理而不是回答，和拆分任务进行引导**。

#### 推理而不是回答

推理而不是回答是指在 Prompt 中，要求 GPT 模型不要立即判断正确与否或者立刻给出答案，而是引导模型进行深入思考。可以要求其先列出对问题的各种看法，对任务进行拆分，说明每一步的推理依据，然后再得出最终结论。在 Prompt 中添加逐步推理的要求，能让语言模型投入更多时间逻辑思维，输出结果也将更可靠准确。

举一个[OpenAI 官方的例子](https://platform.openai.com/docs/guides/gpt-best-practices/tactic-instruct-the-model-to-work-out-its-own-solution-before-rushing-to-a-conclusion)，如果你需要 GPT 回答某一个学生的答案是否正确，Prompt 是 `判断学生的解决方案是否正确`的话，面对复杂的计算问题和答案，GPT 有很大的概率会给出错误的答案，因为 GPT 并不会像人一样先进行推理答案再进行回答，而是会立即给出判断。在短暂的判断中，就无法给出正确的答案（就好比人类无法在短时间计算复杂数学一样）。

所以如果我们的 Prompt 是 `首先自己解决问题，然后再将自己的解决方案与学生的解决方案进行比较，并评估学生的解决方案是否正确。在自己完成问题之前，不要确定学生的解决方案是否正确`。在 Prompt 中给出明确的引导和条件，就能够让 GPT 模型花费更多的时间推导答案，从而得到更加准确的结果。

#### 拆分任务

拆分任务进行引导是指将一个复杂的任务拆分成多个子任务，然后分别引导 GPT 模型进行推理，最后将多个子任务的结果进行整合，得到最终的结果。这样做的好处是可以让 GPT 模型更加专注于一个子任务，从而提高输出质量。

举一个最简单的的例子，当你需要对一本书籍进行摘要的时候，GPT 直接进行摘要的效果并不好，我们可以使用一系列子任务来摘要每个部分。最后再汇总产生的摘要。

当然拆分任务也会带来一些新的问题，即当单个任务输出质量有问题时，整体的输出质量也会受其影响。加上目前的 token 费用不菲，拆分任务进行引导也会带来额外的成本。但是无论如何，目前关于如何设计和拆分复杂任务是所有 GPT 应用最需要思考，和维护自身 AI 应用护城河的核心问题，也是目前大模型 AI 框架像 LangChain 等项目的核心设计点，有空可以单独写一篇文章来讨论。

### 使用技巧

除了上述的一些比较重要的实践之外，还有一些小技巧对于开发应用也是非常有帮助，下面是笔者总结的一些使用技巧：

- 提供少量示例(Few-Shot Prompting): 给模型一两个期望的输入输出样例，让模型了解我们的要求和期望的输出样式。
- Prompt 中要求结构化输出：以 Json 的方式输出，这样可以方便后续代码程序处理。
- 分隔符：使用分隔符像 `"""` 将不同的指令、上下文之间进行隔离，防止系统的 Prompt 和用户输入的 Prompt 混淆冲突。

## GPT Embedding 应用开发

在上面我们主要介绍了基于 Prompt 如何开发 AI 应用，但是有时候我们还会遇到新的问题。例如大模型的训练数据往往是基于几个月前、甚至是几年前的，当我们有一些需求场景是需要 GPT 应用提供最新的数据，例如基于最近的新闻来回答问题，或者根据私有文档来回答问题，这时候大模型本身就没有基于这些材料进行过训练，也就无法解决该类问题。

这个时候我们可以通过让模型使用参考文本来回答问题，例如我们的 prompt 可以写成:

```
你将会得到一个由三个引号分隔的文档和一个问题。

你的任务是使用提供的文档回答问题，并引用用于回答问题的文本段落。

如果文档中没有包含回答该问题所需的信息，则简单地写下：“信息不足”。

如果提供问题的答案，必须附带引用注释。
使用以下格式引用相关段落（{"citation": …}）。
```

这种使用方式可以让 GPT 针对于我们给予它的参考文本进行回答，例如我们想问最新的世界杯冠军是谁，就可以附上最新的世界杯新闻的参考文本，这样 GPT 会先理解整篇新闻，再来问答问题。通过这种方式，就可以解决大模型针对于时效性和特定的下游任务的问题。

但是这种解决方案会带来另外一个问题，即参考文本长度限制的问题。GPT Prompt 是有大小限制的，像 gpt-3.5-turbo 模型它的限制是 4K tokens(～3000字)，这意味着使用者最多只能输入 3000 字给 GPT 来理解和推理答案。

那么一旦需要的参考文本大于 3000 字，就无法一次性通过 GPT 得到答案，需要开发者想办法将参考文本拆分成多个部分，然后分别进行 GPT Embeddings 转化为向量存储到向量数据库中。关于向量数据库的更多信息，可以参考我另外一篇博客 [向量数据库]({{< ref "blog/zh/vector-database" >}}) ，当需要针对于参考文本进行提问时，需要先将问题转化为向量，然后通过向量数据库进行检索，最后再将检索到的向量转化为文本输出。这样就能得到符合 GPT tokens 限制的参考文本，并且这段参考文本是和问题具有关联的。

我们拿到问题本身和这段具有关联的参考文本，同时提交给 GPT，就能得到我们想要的答案。这个过程就是 GPT Embeddings 应用开发核心，它的核心思想是通过向量检索的方式检索与问题最相关的文本段，从而绕过 GPT tokens 的限制。

![Embedding](https://storage.guangzhengli.com/images/Embedding.png)

整体的开发流程如上图所示：
1. 加载文档，获取目标文本信息。例如 LLM 主流框架 LangChain 中 File Loader 和 Web Loader 的两种文本获取方式。
   1. 基于文件系统的 File Loader，如加载 PDF 文件、Word 文件等。
   2. 基于网络的 Web Loader，如某个网页，AWS S3 等。
2. 将目标文本拆分为多个段落，拆分方式主要基于两种。
    1. 一种是基于文字数量的拆分法，例如 1000 字为一段，这种方式的优点是简单，缺点是可能会将一个段落拆分成多个段落，导致段落的连贯性变差，从而导致最终回答结果可能缺少上下文而降低回答质量。
    2. 另一种是基于标点符号的拆分法，例如以换行为分隔符，这种方式的优点是段落的连贯性好，缺点是每个段落大小不一，可能会导致触发 GPT tokens 的限制。
    3. 最后一种是基于 GPT tokens 限制的拆分法，例如以 2000 tokens 为一组，最终查询时搜索出最相关的两个段落，这样加到一起也才 4000 tokens,就不会触发到 4096 tokens 的限制。
3. 将拆分的文本块统一存入 [向量数据库]({{< ref "blog/zh/vector-database" >}}) 中。
4. 将用户问题转化为向量，然后通过向量数据库进行检索，得到最相关的文本段落。（需要注意的是，这里的检索并不是传统数据库的模糊匹配或者倒排索引，而是基于语义化的搜索能力，所以检索出的文本才能回答用户问题，详细请看另外一篇博客 [向量数据库]({{< ref "blog/zh/vector-database" >}}）
5. 将检索出来的相关文本信息，用户问题和系统的 prompt 三个组合成一个针对于 Embeddings 场景的 Prompt，例如 Prompt 中明确写到使用参考文本回答问题，而不是 GPT 自己回答。
6. GPT 回答最终得到的 Prompt，得到最终的答案。

GPT embeddings 应用能解决某些场景下 GPT 无法回答的问题，这是他最大的优点。无需训练，无需微调，只需要将文本转化为向量再通过检索就能实现，以低成本的方式就能让某些业务场景变成可能。

但 GPT embeddings 方案同样会带来一些新的问题，例如 embeddings 文本拆分和检索的质量将在很大的程度上影响最终的结果。例如参考文本本身的质量问题，在原来，所有的文档都是面向人类编写，以后是否会存在专门面向 AI 编写的文本，让 AI 更好理解和更好符合数据库检索？这都是需要长期思考的问题。

## GPT 应用需求分析

上面是关于如何开发 GPT 应用的一些思考和技巧，但是如果想要创造产品，我们还是需要从业务需求出发，去思考我们能够创造出什么样的业务价值，从而满足当前潜在用户的需求。

### 非结构化输入和结构化输出

首先我能想到的一点就是处理非结构化的数据变得简单很多，例如在原来，我们想要处理一些简单的文本，例如从短信中提取出一些关键信息，例如姓名、电话号码、地址等等，由于不同的短信模版不同，所以这些信息都是非结构化。

我们无法通过一个轻易的方法将这些内容转成代码中的对象从而结构化输出，所以我们往往需要通过一套复杂的正则表达式，或者 NLP 技术来将这些信息提取出来。但是一是由于正则表达式开发复杂，而且对于不同的短信模版和时间上的变化，我们需要不断的调整正则表达式，这个过程是需要消耗大量精力的。

NLP 技术又只能针对于某一些特定的场景，例如提取电话号码，提取地址等等，对于不同的场景，我们需要不同的 NLP 技术，所以一旦业务需求发生了变化，例如识别车牌，我们还需要重新开发和调整。

但是有了 GPT 统一的 API，我们只需要给服务提供不同的 Prompt，就可以通过 Prompt 来引导 GPT 进行推理，从而得到我们想要的结果。这样就能够大大的简化我们的开发流程和缩短开发时间，从而快速的响应市场的变化。

并且由于 GPT 的强大的泛化能力，我们只需要针对于不同的场景，思考好如何处理非结构化数据即可，例如从 PDF 文件中，就可以直接拿到相关信息回答提出的问题；再比如从 Microsoft Word 文件中获取相关关键数据预填表单；甚至还可以直接将客户问题直接转化为 SQL 查询数据库。

这种处理非结构化数据的能力在未来软件开发中会改变很多以往业务的开发流程和方式，将很多以往想都不敢想的功能变成现实。

## References

- https://github.com/guangzhengli/ChatFiles
- https://github.com/guangzhengli/vectorhub